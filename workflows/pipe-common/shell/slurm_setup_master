#!/bin/bash

# Copyright 2017-2019 EPAM Systems, Inc. (https://www.epam.com/)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

SLURM_MASTER_SETUP_TASK="SLURMMasterSetup"
SLURM_CLUSTER_SETUP_TASK="SLURMClusterSetup"
SLURM_MASTER_SETUP_TASK_WORKERS="SLURMMasterSetupWorkers"
CURRENT_PID=$$

configure_slurm() {

mkdir /var/spool/slurmctld
chown slurm: /var/spool/slurmctld
chmod 755 /var/spool/slurmctld
touch /var/log/slurmctld.log
chown slurm: /var/log/slurmctld.log
touch /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log
chown slurm: /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log

    cat > /common/slurm.conf <<EOL
# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=$HOSTNAME
#
#MailProg=/bin/mail
MpiDefault=none
#MpiParams=ports=#-#
ProctrackType=proctrack/pgid
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
StateSaveLocation=/var/spool/slurmctld
SwitchType=switch/none
TaskPlugin=task/none

# SCHEDULING
FastSchedule=1
SchedulerType=sched/backfill
SelectType=select/linear

# LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/none
ClusterName=buhpc
JobAcctGatherType=jobacct_gather/none
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdLogFile=/var/log/slurmd.log
#
#
# COMPUTE NODES
NodeName=$HOSTNAME State=UNKNOWN
EOL
}

get_linux_dist() {
    result=
    command -v apt-get > /dev/null
    if [ $? -eq 0 ]; then
        result="debian"
    fi

    command -v yum > /dev/null
    if [ $? -eq 0 ]; then
        result="redhat"
    fi

    echo "$result"
}

check_last_exit_code() {
   exit_code=$1
   msg_if_success=$2
   msg_if_fail=$3
   if [[ "$exit_code" -ne 0 ]]; then
        pipe_log_fail "$msg_if_fail" "${SLURM_MASTER_SETUP_TASK}"
        kill -s "$CURRENT_PID"
        exit 1
    else
        pipe_log_info "$msg_if_success" "${SLURM_MASTER_SETUP_TASK}"
    fi
}

LINUX_DISTRIBUTION=$( get_linux_dist )


pipe_log_info "Installing SLURM master" "$SLURM_MASTER_SETUP_TASK"

if [ "$LINUX_DISTRIBUTION" = "debian" ]; then
    apt install munge slurm-wlm
    _SLURM_CONFIG_LOCATION=/etc/slurm-llnl/
    mkdir -p /var/run/munge
    chown munge: /var/run/munge
elif [ "$LINUX_DISTRIBUTION" = "redhat" ]; then
    _SLURM_CONFIG_LOCATION=/etc/slurm/
    export SLURMUSER=992
    groupadd -g $SLURMUSER slurm
    useradd  -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm  -s /bin/bash slurm

    yum install mariadb-server mariadb-devel epel-release  munge munge-libs munge-devel openssl openssl-devel \
                pam-devel numactl hwloc hwloc-devel lua lua-devel readline-devel rrdtool-devel ncurses-devel  \
                man2html libibmad libibumad rpm-build perl-devel perl-CPAN -y

    wget https://download.schedmd.com/slurm/slurm-19.05.5.tar.bz2
    rpmbuild -ta slurm*.tar.bz2
    mkdir /common/slurm-pkgs
    cp /root/rpmbuild/RPMS/x86_64/* /common/slurm-pkgs/
    yum --nogpgcheck localinstall /common/slurm-pkgs/* -y
fi

configure_slurm

dd if=/dev/urandom bs=1 count=1024 > /common/munge.key
cp /common/munge.key /etc/munge/
chown munge: /etc/munge/munge.key
chmod 400 /etc/munge/munge.key
su -c /usr/sbin/munged -s /bin/bash munge

pipe_log_success "Master ENV is ready" "$SLURM_MASTER_SETUP_TASK"


# Verify whether it is a resumed run - if so, do not reinstall sge, instead - just rerun master and exec daemon
if [ "$RESUMED_RUN" = true ]; then
    pipe_log_info "Run is resumed - SLURM master node won't be reconfigured. Starting master and worker daemons" "$SLURM_MASTER_SETUP_TASK"
    slurmd -f /etc/slurm/slurm.conf && slurmctld -f /etc/slurm/slurm.conf
    _SLURM_RESUME_RESULT=$?

    if [ $_SLURM_RESUME_RESULT -eq 0 ]; then
        pipe_log_success "SLURM daemons started" "$SLURM_MASTER_SETUP_TASK"
    else
        pipe_log_fail "SLURM daemons start failed. See any errors in the ConsoleOutput" "$SLURM_MASTER_SETUP_TASK"
    fi
    exit 0
fi

# Wait for worker nodes to initiate and connect to the master
if [ -z "$node_count" ] || (( "$node_count" == 0 )); then
    pipe_log_success "Worker nodes count is not defined. Won't wait for execution hosts" "$SLURM_MASTER_SETUP_TASK_WORKERS"
else
    _MASTER_EXEC_WAIT_ATTEMPTS=${_MASTER_EXEC_WAIT_ATTEMPTS:-60}
    _MASTER_EXEC_WAIT_SEC=${_MASTER_EXEC_WAIT_SEC:-10}
    _CURRENT_EXEC_HOSTS_COUNT=$(( $(cat /common/slurm.conf | grep 'NodeName=' | wc -l) - 1))
    while [ "$node_count" -gt "$_CURRENT_EXEC_HOSTS_COUNT" ]; do
        pipe_log_info "Waiting for execution hosts to connect. $_CURRENT_EXEC_HOSTS_COUNT out of $node_count are ready" "$SLURM_MASTER_SETUP_TASK_WORKERS"
        sleep $_MASTER_EXEC_WAIT_SEC
        _CURRENT_EXEC_HOSTS_COUNT=$(( $(cat /common/slurm.conf | grep 'NodeName=' | wc -l) - 1))
        _MASTER_EXEC_WAIT_ATTEMPTS=$(( _MASTER_EXEC_WAIT_ATTEMPTS-1 ))

        if (( $_MASTER_EXEC_WAIT_ATTEMPTS <= 0 )); then
            pipe_log_success "NOT all execution hosts are connected. But we are giving up waiting as threshold has been reached" "$SLURM_MASTER_SETUP_TASK_WORKERS"
            exit 0
        fi
    done
    pipe_log_success "All SLURM hosts are connected" "$SLURM_MASTER_SETUP_TASK_WORKERS"
fi

_NODE_NAMES=`cat /common/slurm.conf | grep -Po 'NodeName=[\w\-_\[\]]+' | cut -d'=' -f2| xargs |  awk -v OFS="," '{$1=$1;print}'`
echo "PartitionName=main.q Nodes=$_NODE_NAMES Default=YES MaxTime=INFINITE State=UP" >> /common/slurm.conf
cp /common/slurm.conf "$_SLURM_CONFIG_LOCATION"

slurmctld && slurmd

pipe_log_success "SLURM cluster READY!" "$SLURM_CLUSTER_SETUP_TASK"
